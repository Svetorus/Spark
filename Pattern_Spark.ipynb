{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pass.txt\", \"r\") as f:\n",
    "    login = !whoami\n",
    "    login = login[0]\n",
    "    passwd = f.read()\n",
    "!echo $passwd | kinit $login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылка на простые действия спарк\n",
    "https://pythonru.com/biblioteki/pyspark-dlja-nachinajushhih"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чтение данных из источника\n",
    "Основной метод чтения любых источников\n",
    "\n",
    "```df = spark.read.format(datasource_type).option(datasource_options).load(object_name)```\n",
    "\n",
    "+ ```datasource_type``` - тип источника (\"parquet\", \"json\", \"cassandra\") и т. д.\n",
    "+ ```datasource_options``` - опции для работы с источником (логины, пароли, адреса для подключения и т. д.)\n",
    "+ ```object_name``` - имя таблицы/файла/топика/индекса\n",
    "\n",
    "[DataframeReader](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader):\n",
    "+ по умолчанию выводит схему данных\n",
    "+ является трансформацией (ленивый)\n",
    "+ возвращает [Dataframe](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "\n",
    "### Список (неполный) поддерживаемых источников данных\n",
    "+ Файлы:\n",
    "  - json\n",
    "  - text\n",
    "  - csv\n",
    "  - orc\n",
    "  - parquet\n",
    "  - delta\n",
    "+ Базы данных\n",
    "  - elasticsearch\n",
    "  - cassandra\n",
    "  - jdbc\n",
    "  - hive\n",
    "  - redis\n",
    "  - mongo\n",
    "+ Брокеры сообщений\n",
    "  - kafka\n",
    "  \n",
    "\n",
    "**Библиотеки для работы с источниками должны быть доступны в JAVA CLASSPATH на драйвере и воркерах!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import datetime\n",
    "from sys import argv\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import HiveContext, SQLContext, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from ### import vertica\n",
    "from ###.sparksession_helper import get_spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запустить спарк сессию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запуск \n",
    "spark_session = get_spark_session(template=\"MEDIUM_30\",\n",
    "                                  app_name=\"Answer_email\",\n",
    "                                  max_live_hours=10, \n",
    "                                  vrt_jdbc_support = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Открыть файл CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# открыть CSV формат spark \n",
    "df = spark_session.read.format(\"csv\") \\\n",
    "        .options(header=True, inferSchema=True) \\\n",
    "        .load(\"/tmp/datasets/airport-codes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Открыть файл Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('/data/stats/maxima/airflow_files/stations_stat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Открыть файл AVRO в нескольких папках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = config['files']['pcs']['root_path']\n",
    "filenames = config['files']['pcs']['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gendates(daycnt, sevodnya=False):\n",
    "    \"\"\"\n",
    "    Генерирует пути для чтения avro-файлов\n",
    "    Args:\n",
    "        daycnt: количество дней\n",
    "        sevodnya: нужно ли учитывать сегодняшний день\n",
    "\n",
    "    Returns: генератор дня, месяца, года\n",
    "\n",
    "    \"\"\"\n",
    "    if sevodnya:\n",
    "        td = 0\n",
    "    else:\n",
    "        td = 1\n",
    "    for i in range(td, daycnt + 1):\n",
    "        # a = datetime(2021,9,21,12) - timedelta(days=i)\n",
    "        a = datetime.today() - timedelta(days=i)\n",
    "        yield a.day, a.month, a.year\n",
    "\n",
    "\n",
    "def genpathes(root_path, daycnt, sevodnya=False):\n",
    "    \"\"\"\n",
    "    Генерирует пути для чтения avro-файлов\n",
    "    Args:\n",
    "        root_path: корневая директория\n",
    "        daycnt: количество дней\n",
    "        sevodnya: нужно ли учитывать сегодняшний день\n",
    "\n",
    "    Returns: массив путей\n",
    "\n",
    "    \"\"\"\n",
    "    return [path.join(root_path, f'year={year}/month={month:02}/day={day:02}/')\n",
    "            for day, month, year in gendates(daycnt, sevodnya)]\n",
    "\n",
    "\n",
    "def genmailpathes(root_path, filename, daycnt, sevodnya=False):\n",
    "    \"\"\"\n",
    "    Генерирует пути для каталогов из filename внутри root_path\n",
    "    Args:\n",
    "        root_path: корневая директория\n",
    "        filename: тип рассылки\n",
    "        daycnt: количество дней\n",
    "        sevodnya: нужно ли учитывать сегодняшний день\n",
    "\n",
    "    Returns: массив путей\n",
    "\n",
    "    \"\"\"\n",
    "    mailpathes = {}\n",
    "    for folder in filename:\n",
    "        full_path = root_path + folder\n",
    "        mailpathes[folder] = genpathes(full_path, daycnt, sevodnya)\n",
    "    return mailpathes\n",
    "\n",
    "def readavro(spark_session, root_path, path2avro):\n",
    "    \"\"\"\n",
    "    Проверяем список путей на наличие пропусков. Очищенный список подаем на чтение\n",
    "    Args:\n",
    "        spark_session: спарк-сессия\n",
    "        root_path: базовый путь каталога с данными\n",
    "        path2avro: список путей до авро-файла\n",
    "    Returns: датафрейм\n",
    "    \"\"\"\n",
    "    list2read = []\n",
    "    for i in path2avro:\n",
    "        list2read.append(i)\n",
    "    if list2read:\n",
    "        dfavro = spark_session.read \\\n",
    "            .option('basePath', root_path) \\\n",
    "            .format('avro').load(list2read)\n",
    "        return dfavro\n",
    "    else:\n",
    "        logging.warning('Часть данных отсутствует')\n",
    "\n",
    "\n",
    "def prepareevents(spark_session, root_path, dict_pathes):\n",
    "    \"\"\"\n",
    "    Обрабатываем датафрейм для каталога 'mail_events' (событие создания рассылки)\n",
    "    Args:\n",
    "        spark_session: спарк-сессия\n",
    "        root_path: базовый путь каталога с данными\n",
    "        dict_pathes: словарь нагенеренных путей к каталогу\n",
    "\n",
    "    Returns: готовый датафрейм\n",
    "    \"\"\"\n",
    "    df = readavro(spark_session, root_path, dict_pathes['mail_event'])\n",
    "    if df is not None:\n",
    "        return df \\\n",
    "            .select('event_uuid', 'event_time', col('contact_to').alias('email'),\n",
    "                    'topic', 'mailing_id', 'campaign_id') \\\n",
    "            .where((col('topic') != '') & (col('stream_type') == 'email'))\n",
    "    else:\n",
    "        schema = StructType([\n",
    "            StructField('event_uuid', StringType(), True),\n",
    "            StructField('event_time', LongType(), True),\n",
    "            StructField('email', StringType(), True),\n",
    "            StructField('topic', StringType(), True),\n",
    "            StructField('mailing_id', LongType(), True),\n",
    "            StructField('campaign_id', LongType(), True)\n",
    "        ])\n",
    "        return spark_session.createDataFrame([], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = genmailpathes(root_path, filenames, 28, False)\n",
    "# можно удалить из словаря даты которой еще нет в списке\n",
    "#dicts['mail_open'].remove('/data/stats/pcs/landing/mail_open/year=2021/month=09/day=05/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = spark.read \\\n",
    "            .option('basePath', root_path) \\\n",
    "            .format('avro').load(dicts['mail_event'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Открыть файл Excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# исходный файл сохранила в обычном формате xlsx\n",
    "df_orig = pd.read_excel('file.xlsx', engine='openpyxl', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  open in spark session file xlsx\n",
    "df_spark = spark_session.createDataFrame(df_orig.astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Записать в  файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## сохраняю результат в csv.\n",
    "df.repartition(1).write.mode('overwrite').csv('./file.csv', encoding='windows-1251', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запускаем Vertica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запускаем VERTICA для открытия таблиц\n",
    "KERBEROS_VERTICA = {\n",
    "    \"user\": login,\n",
    "    \"password\": passwd,\n",
    "    \"db\": \"verticadb\",\n",
    "    \"hdfs_url\": \"hdfs://nameservice1/user/vertica-udrvs/\", \n",
    "    \"url\": \"jdbc:vertica://udrvs-ver-1x-cluster.data.corp:5433/verticadb\",\n",
    "    \"host\": \"udrvs-ver-1x-cluster.data.corp\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elk_main = vertica.get_table_via_jdbc(tablename='main', scheme='data', \\\n",
    "                                      spark_session=spark_session, opts=KERBEROS_VERTICA)\\\n",
    ".select('ID', 'EMAIL', 'uuid') # выбираем колонки для отображения\n",
    "# tablename - имя таблицы\n",
    "# scheme - имя схемы\n",
    "# spark_session - спарк-сессия\n",
    "# opts - конфигурация соединения (см. выше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `show` выводит часть датафрейма в консоль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вывод данных \n",
    "df.show(10, False)\n",
    "# Дополнительный параметр vertical=True выведет каждую строку данных по строчно ввиду колонка|значение\n",
    "df.show(n=1, truncate=False, vertical=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# показать таблицу с Null в колонке col1\n",
    "from pyspark.sql import functions as F\n",
    "df.where(F.col('col1').isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `select` позволяет выбрать существующие (а также создать новые) колонки из датафрейма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"continent\", \"country\").show(10, False)\n",
    "df.localCheckpoint().select(\"continent\", \"country\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим дубликаты. По умолчанию метод dropDuplicates удаляет дубликаты строк, у которых ВСЕ колонки совпадают\n",
    "df.dropDuplicates(subset=[\"name\"]).show(10, False)\n",
    "df.localCheckpoint().dropDuplicates(subset=[\"name\"]).explain()\n",
    "# Метод .na.drop удаляет СТРОКИ, в которых отсутствует часть данных. Параметр how=\"all\" означает,\n",
    "# что будут удалены строки, у которых ВСЕ колонки null\n",
    "df.dropDuplicates().na.drop(how=\"all\").show(10, False)\n",
    "df.localCheckpoint().dropDuplicates().na.drop(how=\"all\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод .na.fill заполняет null. Для работы этого метода требуется словарь с изменениями\n",
    "fill_dict = {'continent': 'n/a', 'population': 0 }\n",
    "\n",
    "df.dropDuplicates().na.drop(how=\"all\").na.fill(fill_dict).show(10, False)\n",
    "df.localCheckpoint() \\\n",
    "    .dropDuplicates() \\\n",
    "    .na.drop(how=\"all\") \\\n",
    "    .na.fill(fill_dict).explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод .na.replace заменяет данные в колонках. Для его работы требуется словарь с заменами\n",
    "replace_dict = {\"Rossiya\": \"Russia\"}\n",
    "\n",
    "df.dropDuplicates().na.drop(\"all\").na.fill(fill_dict).na.replace(replace_dict).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовим базовый агрегат. По умолчанию имена колонок принимают неудобные названия\n",
    "from pyspark.sql.functions import count, sum\n",
    "\n",
    "agg = clean_data.groupBy(\"continent\").agg(count(\"*\"), sum(col(\"population\")))\n",
    "agg.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод alias позволяет переименовывать колонки\n",
    "from pyspark.sql.functions import count, sum, lower\n",
    "\n",
    "pop_count = count(\"*\").alias(\"city_count\")\n",
    "pop_sum = sum(col(\"population\")).alias(\"population_sum\")\n",
    "\n",
    "agg = clean_data \\\n",
    "            .groupBy(\"continent\") \\\n",
    "            .agg(pop_count, pop_sum) \\\n",
    "            .withColumn(\"continent\", lower(col(\"continent\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нижний регистр\n",
    "df.withColumn(column, F.lower(col(column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводим нужные колонки вариант SQL\n",
    "df.select('uuid','vehicle_brand','vehicle_year').show(5)\n",
    "# Выводим нужные колонки вариант DataFrame\n",
    "df.select(df['uuid'],df['vehicle_brand'],df['vehicle_year']).show(5)\n",
    "# Удобнее и логичнее использовать col() для составления условий фильтрации\n",
    "df.select(F.col('uuid'),F.col('vehicle_brand'),F.col('vehicle_year')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводим нужные колонки вариант SQL\n",
    "df\\\n",
    "    .select('uuid','vehicle_brand','vehicle_year')\\\n",
    "    .filter(F.col(\"vehicle_brand\") == 'Бмв 328i Хdrivе')\\\n",
    "    .filter(F.col(\"vehicle_year\") == '2013')\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"vehicle_brand\").count().show(5)\n",
    "df.groupBy(\"vehicle_brand\").count().orderBy(\"count\").show(5)\n",
    "df.groupBy(\"vehicle_brand\").count().orderBy(F.col(\"count\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение колонки на две по знаку\n",
    "from pyspark.sql.functions import split\n",
    "df.select(split(F.col('email'), '@')[0].alias('Мыло')\n",
    "# Разделение колонки на две по знаку для всей таблицы\n",
    "df_spark.withColumn('ДобавочныйНомер',split(df_spark[col_14],'доб').getItem(1)).show(100,False,True)\n",
    "# вариант создания функции для резделения\n",
    "split_col = pyspark.sql.functions.split(df['my_str_col'], '-')\n",
    "df = df.withColumn('NAME1', split_col.getItem(0))\n",
    "df = df.withColumn('NAME2', split_col.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вариант дублирования колонки 'column'\n",
    "df.withColumn('NAME1',col('column')).show(100,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбираем данные с индекса 5 по 7 и записываем в новую колонку\n",
    "df.select(col('column').substr(5, 7).alias(\"NewNameColumn\")).show(5,False,True)\n",
    "df.withColumn(column, col(column).substr(0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обрезает и выводит с 1 позиции до 12\n",
    "df.select(col('column').substr(1, 12)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаляем в начале только символ 7 на пустоту\n",
    ".withColumn(col_14,F.regexp_replace(col_14, '^[7]',''))\\\n",
    "# выбираем начало на цифры в списке и заменяем всё значение на 'Нет'\n",
    ".withColumn(col_14, F.when(col(col_14).rlike(\"^[0123567]\"), 'Нет').otherwise(col(col_14)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter AND (&), OR (|) и NOT (~)\n",
    "df.filter( (col('data') >= lit('2020-01-01')) & (col('data') <= lit('2020-01-31')) ).show(5)\n",
    "# Between\n",
    "df.filter(data.adjusted.between(100.0, 500.0)).show()\n",
    "# Он возвращает 0 или 1 в зависимости от заданного условия. В приведенном ниже примере показано, \n",
    "#как выбрать такие цены на момент открытия и закрытия торгов, при которых скорректированная цена была больше или равна 200\n",
    "data.select('open','close',f.when(data.adjusted >= 200.0, 1).otherwise(0)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# убираем дубли в строке\n",
    "from pyspark.sql.functions import udf\n",
    "remove = udf(lambda row: (sorted(set(row.split(' ')), key=row.index)), returnType=StringType())\n",
    "df_join = udf(lambda row: (\" \".join(row)), returnType=StringType())\n",
    "df_spark.withColumn(col_4, remove(col_4)).select(col_4)\\\n",
    "    .withColumn(col_4, df_join(col_4)).select(col_4)\\\n",
    "    .filter(col(col_4).rlike('Барсков')).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(F.col('mailing_id')==139543)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# указываем колонку для заполения null и заменям значением unknown\n",
    "df.na.fill('UNKNOWN', \"column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('open','click').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выводим кол-во символов в каждой строчке и записываем в новую колонку\n",
    "df.select(length('column').alias('length')).show(100,False,True)\n",
    "# выводим длину строчек в колонке\n",
    "df.select(length('column').alias('length')).distinct().show()\n",
    "# выводим длину строчек\n",
    ".select('column').filter(length('column')==6).distinct().show()\n",
    "# выводим варианты длины строки и кол-во\n",
    ".select(length('column').alias('Длина')).groupBy('Длина').count().show()\n",
    "# выбираем длину цифр меньше 9 и больше 11 и заменяем всё значение на 'Нет' все остальные значения оставляем\n",
    ".withColumn(col_14, F.when(length(col_14) < 9, 'Нету').when(length(col_14) > 11, 'Нет').otherwise(col(col_14)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ev \\\n",
    "        .join(opend.select('uuid').withColumn('open', F.lit(1)), 'uuid', 'left') \\\n",
    "        .join(send.select('uuid').withColumn('send', F.lit(1)), 'uuid', 'left') \\\n",
    "        .join(undel.select('uuid').withColumn('undel', F.lit(1)), 'uuid', 'left') \\\n",
    "        .join(click.select('uuid').withColumn('click', F.lit(1)), 'uuid', 'left') \\\n",
    "        .fillna(0, subset=['undel', 'open', 'click', 'resp']) \\\n",
    "        .select('event_uuid', 'contact_to', 'undel', 'open', 'click','event_time')\\\n",
    "        .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join варианты\n",
    "df = df_1.join(df_2, df_1.SSO_ID==df_2.SSO_ID, 'left')\n",
    "df = df_1.join(df_2, \"SSO_ID\", 'left')\n",
    "\n",
    "cond = [df.name == df3.name, df.age == df3.age]\n",
    "df = df_1.join(df_2, cond, 'left')\n",
    "\n",
    "cond = [df.surname == df3.surname, (df.name == df3.name) | (df.age == df3.age),\n",
    "        df.orders >= df3.orders ]\n",
    "df.join(df3, cond, 'outer')\n",
    "\n",
    "cond = [((df1.col1 == df2.col2) & (df1.col3 == df2.col4))]\n",
    "joined_df = df1.join(df2, on = cond, how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удалить дубликаты\n",
    "df.dropDuplicates(['name', 'height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переименовать существующую колонку 1 на 2\n",
    "df.withColumnRenamed('1','2')\n",
    "# Переименовать существующую колонку .withColumnRename\n",
    "df.withColumnRename(\"vehicle_brand\",\"SUPER_vehicle_brand\").select(\"SUPER_vehicle_brand\").show(5)\n",
    "# Создать колонку .withColumn берем vehicle_year и добавляем 1 год и получаем колонку next_year\n",
    "df.withColumn(\"next_year\",F.col(\"vehicle_year\")+1).select(\"vehicle_year\",\"next_year\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем новую колонку 'col3' где будут колонки 'col1' без Null \n",
    "# и если 'col1' Null то взять значение из колонки 'col2'\n",
    "df.withColumn('col3', F.when(F.col('col1').isNotNull(),F.col('col1')).otherwise(F.col('col2')))\n",
    "# потом проверить пропуски в колонке '3'\n",
    "df.where(F.col('col3').isNull()).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
